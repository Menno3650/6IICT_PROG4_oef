{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-bottom:-35px;\">\n",
    "    <font color=#690027 markdown=\"1\">\n",
    "        <h1> <center> Gebaseerd op een cursus van:</center> </h1> \n",
    "    </font>\n",
    "    <a href=\"https://www.aiopschool.be/chatbot/\"> \n",
    "        <img src=\"../_afbeeldingen/bannerugentdwengo.png\" alt=\"Dwengo\" style =\"display: block; margin-left: auto; margin-right: auto; margin-bottom: 30px; width:20%\"/>\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-bottom:-35px;\">\n",
    "    <font color=#690027 markdown=\"1\">\n",
    "        <h1>1. ML-Model voor Sentimentanalyse</h1> \n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#013220\">\n",
    "In deze Notebook zal je bij gegeven teksten (de data) onderzoek doen naar sentimentwoorden. Je zal hier een ML-model gebruiken om de vervelende stappen uit de vorige Notebook te automatiseren. <br><br> Dit model is getraind met geannoteerde (gelabelde) teksten en kan met grote nauwkeurigheid een tekst <b>tokeniseren</b> en van elk token de <b>woordsoort tag</b> en het <b>lemma</b> bepalen. <br><br> Tenslotte gebruikt een <em>regelgebaseerd systeem</em> deze <b> tags en lemmas</b> om het sentiment van de tekst te bepalen. \n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In de vorige notebook, 'Regelgebaseerde sentimentanalyse', maakte je kennis met de principes van een regelgebaseerde sentimentanalyse: \n",
    "\n",
    " -  Je maakt gebruik van een **lexicon** of woordenboek met daarin woorden gekoppeld aan hun **sentiment** (positief, negatief of neutraal).\n",
    " -  Voor je sentimentwoorden uit een lexicon kunt matchen met de gegeven tekst moet je deze tekst inlezen en **voorverwerken (preprocessing)**. Veelvoorkomende preprocessing stappen zijn **lowercasing**, **tokenisering**, **woordsoort tagging** en  **lemmatisering**.\n",
    " \n",
    "In de vorige notebook waren nog niet alle stappen geautomatiseerd. Lemmatisering en woordsoort tagging moesten manueel gebeuren. Zoals te zien is op onderstaand stappenplan."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"../_afbeeldingen/AI_sentiment.png\" alt=\"Banner\" style =\"display: block; margin-left: auto; margin-right: auto; margin-bottom: 10px; width:80%\"/>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-success\">\n",
    "In deze notebook zal je de uitvoer van de sentimentanalyse volledig <b>automatiseren</b>. Je zal m.a.w. de computer het werk laten doen: de computer zal de data preprocessen (voorverwerken) met een <em>machine learning-model (ML-model)</em>, en met een <em>regelgebaseerd AI-systeem</em> de <b>lemmas</b> matchen met het gegeven lexicon. Tenslotte neemt de AI op basis van de score een beslissing over het sentiment van de gegeven tekst. Onderstaand stappenplan toont het nieuwe proces.\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"../_afbeeldingen/ML_sentiment.png\" alt=\"Banner\" style =\"display: block; margin-left: auto; margin-right: auto; margin-bottom: 10px; width:80%\"/>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='color: #690027;margin-bottom:-35px;'>\n",
    "    <h2>2. Modules installeren & importeren</h2> \n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-bottom:-35px;\">\n",
    "    <font color=#690027 markdown=\"1\">\n",
    "        <h3>2.1 Installeren ML-model spacy</h3> \n",
    "    </font>\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voor het **tokeniseren**, **taggen** en **lemmatiseren** wordt beroep gedaan op <a href=\"https://spacy.io/\">**spacy**</a>. Dit is een **Natural Language Processer (NLP)** met als doel het herkennen van woorden. Dit voor talen als Engels, Frans, Nederlands, ... <br><br>Vooraleer we **spacy** kunnen gebruiken, moeten we deze eerst installeren in Python."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doorloop volgende code-cellen om de woordsoort tagging toe te passen op ieder token.\n",
    "<div style=\"background-color:#8B0000\"> \n",
    "Wees zeker dat je rechtsboven in de juiste environment zit vooraleer spacy te installeren.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.5.0-cp311-cp311-win_amd64.whl (12.2 MB)\n",
      "     ---------------------------------------- 12.2/12.2 MB 1.2 MB/s eta 0:00:00\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp311-cp311-win_amd64.whl (18 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp311-cp311-win_amd64.whl (91 kB)\n",
      "     ---------------------------------------- 91.9/91.9 kB 1.7 MB/s eta 0:00:00\n",
      "Collecting thinc<8.2.0,>=8.1.0\n",
      "  Downloading thinc-8.1.7-cp311-cp311-win_amd64.whl (1.3 MB)\n",
      "     ---------------------------------------- 1.3/1.3 MB 1.1 MB/s eta 0:00:00\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Using cached wasabi-1.1.1-py3-none-any.whl (27 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.5-cp311-cp311-win_amd64.whl (477 kB)\n",
      "     -------------------------------------- 477.5/477.5 kB 2.1 MB/s eta 0:00:00\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting typer<0.8.0,>=0.3.0\n",
      "  Using cached typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting pathy>=0.10.0\n",
      "  Using cached pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Using cached smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "Collecting tqdm<5.0.0,>=4.38.0\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.5/78.5 kB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\menno\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\menno\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.4-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in c:\\users\\menno\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\menno\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\menno\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (21.3)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "     -------------------------------------- 181.6/181.6 kB 2.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\menno\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\menno\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\menno\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\menno\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\menno\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\menno\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp311-cp311-win_amd64.whl (7.0 MB)\n",
      "     ---------------------------------------- 7.0/7.0 MB 1.1 MB/s eta 0:00:00\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.4-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\menno\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\menno\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\menno\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Installing collected packages: cymem, wasabi, tqdm, spacy-loggers, spacy-legacy, smart-open, pydantic, murmurhash, langcodes, catalogue, blis, typer, srsly, preshed, pathy, confection, thinc, spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 32] Het proces heeft geen toegang tot het bestand omdat het door een ander\n",
      "proces wordt gebruikt: 'C:\\\\Users\\\\Menno\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages\\\\cymem\\\\cymem.cp311-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "c:\\Users\\Menno\\AppData\\Local\\Programs\\Python\\Python311\\python.exe: No module named spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tqdm.exe is installed in 'C:\\Users\\Menno\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script pathy.exe is installed in 'C:\\Users\\Menno\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script spacy.exe is installed in 'C:\\Users\\Menno\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.5.0-cp311-cp311-win_amd64.whl (12.2 MB)\n",
      "     -------------------------------------- 12.2/12.2 MB 928.2 kB/s eta 0:00:00\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Using cached murmurhash-1.0.9-cp311-cp311-win_amd64.whl (18 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.7-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp311-cp311-win_amd64.whl (91 kB)\n",
      "     ---------------------------------------- 91.9/91.9 kB 1.7 MB/s eta 0:00:00\n",
      "Collecting thinc<8.2.0,>=8.1.0\n",
      "  Downloading thinc-8.1.7-cp311-cp311-win_amd64.whl (1.3 MB)\n",
      "     ---------------------------------------- 1.3/1.3 MB 1.2 MB/s eta 0:00:00\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.1-py3-none-any.whl (27 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.5-cp311-cp311-win_amd64.whl (477 kB)\n",
      "     -------------------------------------- 477.5/477.5 kB 1.8 MB/s eta 0:00:00\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting typer<0.8.0,>=0.3.0\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "     -------------------------------------- 48.9/48.9 kB 309.0 kB/s eta 0:00:00\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "     ---------------------------------------- 56.8/56.8 kB 3.1 MB/s eta 0:00:00\n",
      "Collecting tqdm<5.0.0,>=4.38.0\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.5/78.5 kB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\menno\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\menno\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.4-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 1.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in c:\\users\\menno\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\menno\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\menno\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (21.3)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "     -------------------------------------- 181.6/181.6 kB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\menno\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\menno\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\menno\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\menno\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\menno\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\menno\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp311-cp311-win_amd64.whl (7.0 MB)\n",
      "     ---------------------------------------- 7.0/7.0 MB 1.1 MB/s eta 0:00:00\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.4-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\menno\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\menno\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\menno\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Installing collected packages: cymem, wasabi, tqdm, spacy-loggers, spacy-legacy, smart-open, pydantic, murmurhash, langcodes, catalogue, blis, typer, srsly, preshed, pathy, confection, thinc, spacy\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 confection-0.0.4 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.1 preshed-3.0.8 pydantic-1.10.4 smart-open-6.3.0 spacy-3.5.0 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.5 thinc-8.1.7 tqdm-4.64.1 typer-0.7.0 wasabi-1.1.1\n"
     ]
    }
   ],
   "source": [
    "# installeren van spacy (dit kan even duren)\n",
    "!py -m pip install spacy\n",
    "# installeren van Nederlandse bibliotheek\n",
    "!python -m spacy download nl_core_news_sm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vervolgens laden we spacy in en zeggen we welk model deze moet gebruiken. Het **nl_core_news_sm** model is een geleerd (ML) model met een accuraatheid van 93%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'nl_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m#inladen van Nederlandse bibliotheek\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mnl_core_news_sm\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Menno\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\__init__.py:54\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[0;32m     31\u001b[0m     name: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[0;32m     32\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     config: Union[Dict[\u001b[39mstr\u001b[39m, Any], Config] \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     38\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Language:\n\u001b[0;32m     39\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[39m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49mload_model(\n\u001b[0;32m     55\u001b[0m         name,\n\u001b[0;32m     56\u001b[0m         vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[0;32m     57\u001b[0m         disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[0;32m     58\u001b[0m         enable\u001b[39m=\u001b[39;49menable,\n\u001b[0;32m     59\u001b[0m         exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[0;32m     60\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m     61\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Menno\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\util.py:439\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    438\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE941\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname, full\u001b[39m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[39m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 439\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE050\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'nl_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# importeren van spacy\n",
    "import spacy\n",
    "#inladen van Nederlandse bibliotheek\n",
    "nlp = spacy.load(\"nl_core_news_sm\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#013220\">\n",
    "Het lijkt misschien valspelen dat we andermans ML-model gebruiken. Maar onthoud dat je als programmeur vooral praktisch (lees lui) moet zijn. Het opstellen van een eigen ML-model voor tekstherkenning is mogelijk, maar zou ons veel tijd kosten. Je moet namelijk over heel veel gelabelde data bezitten. Bedenk dat je oefen mee 5.3 en 5.4 uit de vorige Notebook maakt, maar dan voor miljoenen woorden...\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-bottom:-35px;\">\n",
    "    <font color=#690027 markdown=\"1\">\n",
    "        <h3>2.1 Inlezen Lexicon</h3> \n",
    "    </font>\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Net als in de vorige Notebook, moeten we opnieuw ons **lexicon** (= woordenboek) inladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lexicon inlezen. Zorg dat het pad correct is (hoofdmap is hfst_5_AI)!\n",
    "with open(\"lexicondict.json\", \"rb\") as file: # bestand lexicondict.json in map data bevat het sentimentlexicon\n",
    "    lexicon = json.load(file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na het uitvoeren van al deze code-cellen ben je klaar voor <font color=#690027 markdown=\"1\">3. Preprocessing</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-bottom:-35px;\">\n",
    "    <font color=#690027 markdown=\"1\">\n",
    "        <h2>3. Preprocessing</h2> \n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#8B8000\"> \n",
    "Merk op dat 3.0 Data inlezen en 3.1 Lowercasing identiek zijn aan 5.0 Data inlezen en 5.1 Lowercasing van de vorige Notebook.\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-bottom:-35px;\">\n",
    "    <font color=#690027 markdown=\"1\">\n",
    "        <h3>3.0 Inlezen review</h3> \n",
    "    </font>\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voor deze opdracht zal je werken met dezelfde **klantenreview** als in de notebook 'Regelgebaseerde sentimentanalyse'. \n",
    "<span id=\"review\"></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"Nieuw concept in Gent, maar dat kan volgens mij toch beter. De meeste cornflakes waren gewoon de basic soorten. Ook wat duur voor de hoeveelheid die je krijgt, vooral met de toppings zijn ze zuinig. En als je ontbijt aanbiedt, geef de mensen dan toch ook wat meer keuze voor hun koffie.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(review)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-bottom:-35px;\">\n",
    "    <font color=#690027 markdown=\"1\">\n",
    "        <h3>3.1 Lowercasing</h3> \n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zet de tekst in de variabele `review` om naar lowercasing. Sla het resultaat op in de variabele `review_kleineletters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zet tekst van de review om naar tekst in kleine letters\n",
    "review_kleineletters = # Vul aan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toon resultaat van lowercasing\n",
    "print(review_kleineletters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-bottom:-35px;\">\n",
    "    <font color=#690027 markdown=\"1\">\n",
    "        <h3>3.2 Tokenisering, Woordsoort tagging en Lemmatisering</h3> \n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In voorgaande Notebook heb je zelf (regelgestuurd) de **tokenisering** geautomatiseerd. het **spacy**-model zal dit echter ook voor zijn rekening nemen (samen met **tagging** en **lemmatisering**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review in kleine letters in model voeren\n",
    "review_voorverwerkt = nlp(review_kleineletters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Van de review zijn nu de **tokens** bepaald en van elk token is de **tag** en **lemma** (woordenboeksoort) opgeslagen in de variabele `review_voorverwerkt`.  <br><br>\n",
    "Je kan het **token**, **tag** en **lemma** als volgt uit deze variabele halen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print het token zelf\n",
    "for token in review_voorverwerkt:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print de woordsoort tag van elk token\n",
    "for token in review_voorverwerkt:\n",
    "    print(token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print de lemma van elk token\n",
    "for token in review_voorverwerkt:\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enkele voorbeelden van de preprocessed (voorwerkte) review zijn:\n",
    "- Het eerste token is 'nieuw'. De basisvorm of lemma is `nieuw` en dit is een adjectief, ADJ.\n",
    "- Neem het token ','. De basisvorm of lemma is `,` en dit is een leesteken, dus een symbool, SYM.\n",
    "- Neem het token 'kan'. De basisvorm of lemma is `kunnen` en dit is een hulpwerkwoord, AUX.\n",
    "- Neem het token 'soorten'. De basisvorm of lemma is `soort` en dit is een substantief, NOUN.\n",
    "\n",
    "<div style=\"background-color:#8B8000\"> \n",
    "Merk op dat <b>**spacy**</b> de woordsoort in het Engels geeft. Dit is ook de reden waarom het **Lexicon** Engelse afkortingen gebruikt.\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oefen mee 3.1:\n",
    "Doorloop `review_voorverwerkt` en stel twee lijsten op `lemmas` en `tags`.\n",
    "- De lijst `lemmas` bevat alle gelemmatiseerde tokens (in correcte volgorde).\n",
    "- De lijst `tags` bevat de woordsoorten van alle tokens (in correcte volgorde).\n",
    "\n",
    "Als voorbeeld:<br>\n",
    "- `review_voorverwerkt` = \"gent , maar dat kan\" <br>\n",
    "- `lemmas` = [\"Gent\", \",\", \"maar\", \"dat\", \"kunnen\"] <br>\n",
    "- `tags` = [\"PROPN\", \"PUNCT\", \"CCONJ\", \"PRON\", \"AUX]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = []\n",
    "tags = []\n",
    "# Vul aan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je kan onderstaande code gebruiken om te controleren of de lijsten juist zijn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toon eerste tien tokens en lemmas/tags van deze tokens\n",
    "for i in range(10):\n",
    "    print(f\"{review_voorverwerkt[i].text} | {lemmas[i]}: {tags[i]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#8B0000\"> \n",
    "Ga pas verder als bovenstaande lijsten correct opgesteld zijn. Vraag na als je niet zeker bent.\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-bottom:-35px;\">\n",
    "    <font color=#690027 markdown=\"1\">\n",
    "        <h2>4. Sentiment lexicon matching</h2> \n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu de review *gepreprocessed* (voorverwerkt) is, kan je het sentiment bepalen met behulp van het sentimentlexicon dat je ter beschikking hebt. Dit was reeds geautomatiseerd in 'Regelgebaseerde sentimentanalyse'. Je neemt de code van 'Regelgebaseerde sentimentanalyse' dus gewoon over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kopieer hier de oplossing van oefen mee 6.1 uit de vorige Notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je kan onderstaande code gebruiken om een zin te printen bij de bekomen sentiment_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eindbeslissing voor deze review\n",
    "if sentiment_score > 0:\n",
    "    sentiment = \"positief\"\n",
    "elif sentiment_score == 0:\n",
    "    sentiment = \"neutraal\"\n",
    "elif sentiment_score < 0:\n",
    "    sentiment = \"negatief\"\n",
    "print(f\"De sentimentscore van de review is: {sentiment_score}\")\n",
    "print(\"Het sentiment van de review is \" + sentiment + \".\")    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oefen mee 4.1:\n",
    "Bekijk de volgende reviews. \n",
    "\n",
    "-  Kies er een uit.\n",
    "-  Welk sentiment koppel jij er intuïtief aan?\n",
    "-  Doorloop alle stappen met de gekozen review (plak nieuwe review <a href=#review>hier</a> en voer alle codeblokken opnieuw uit)\n",
    "- Vergelijk de sentimentscore met je eigen verwachtingen. Komen ze overeen?\n",
    "\n",
    "> Sunrisewater is net wat België nodig heeft. Met het obesitasprobleem dat toch wel aan een opmars bezig is, kunnen we alle initiatieven gebruiken om de jeugd weer gewoon water te laten drinken in plaats van die Amerikaanse bucht! Het smaakt geweldig en wat nog beter is, is dat je het gewoon op elke straathoek kan vinden! Echt geweldig! Vooral de pink and yellow is ten zeerste aan te raden.\n",
    "\n",
    ">  Salé & Sucré staat bekend voor zijn super lekkere en originele cocktails, helaas was er geen alcoholvrije variant te verkrijgen. Onze BOB van dienst moest het dan maar bij frisdrank houden.\n",
    "\n",
    ">  Het was superleuk om eens te mogen proeven van de Filipijnse keuken. De gerechten zaten goed in elkaar, de porties waren zeker groot genoeg en de smaken zaten helemaal goed. Voor herhaling vatbaar!\n",
    "\n",
    ">  Gezellige sfeer, lekkere koffie en een mooi interieur. De combinatie van een studiebar en een babbelbar is een geniaal idee! Studeren met een lekker bakkie koffie, een overheerlijk hapje en samen met andere studenten, werkt enorm motiverend. Het interieur is enorm rustgevend met weinig afleiding, waardoor ik nog nooit zoveel heb kunnen doen!\n",
    "\n",
    "> Wow, wat een coole restaurants! En het eten is er megalekker."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oefen mee 4.2:\n",
    "Je kan ook je eigen review schrijven. Vul de variabele `zelfgeschreven_review` in en voer vervolgens onderstaande codeblok uit. Hoe zal de sentimentanalyse omgaan met sarcasme en cynisme? Komt de analyse overeen met je eigen verwachtingen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plaats zelfgeschreven review tussen aanhalingstekens, pas dus gegeven string aan\n",
    "zelfgeschreven_review = \"Hopelijk was dit een leuke notebook!\"\n",
    "\n",
    "# volgende stappen: review tonen en nlp() erop toepassen\n",
    "print(zelfgeschreven_review)\n",
    "review = nlp(zelfgeschreven_review.lower())\n",
    "\n",
    "# Voeg lemmas en woordsoorten toe aan lijsten\n",
    "lemmas = []\n",
    "woordsoorten = []\n",
    "for index, token in enumerate(review_voorverwerkt):\n",
    "    lemmas.append(token.lemma_)\n",
    "    woordsoorten.append(token.pos_)\n",
    "\n",
    "# Bepaal sentimentscore (oplossing combineert in en get)\n",
    "sentiment_score = 0\n",
    "for index, lemma in enumerate(lemmas):\n",
    "    if lemma not in lexicon:\n",
    "        continue\n",
    "    sentiment_score+= lexicon[lemma].get(woordsoorten[index], 0)\n",
    "\n",
    "# eindbeslissing voor deze review\n",
    "if sentiment_score > 0:\n",
    "    sentiment = \"positief\"\n",
    "elif sentiment_score == 0:\n",
    "    sentiment = \"neutraal\"\n",
    "elif sentiment_score < 0:\n",
    "    sentiment = \"negatief\"\n",
    "print(f\"De sentimentscore van de review is: {sentiment_score}\")\n",
    "print(\"Het sentiment van de review is \" + sentiment + \".\")   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#013220\">\n",
    "Proficiat, je hebt jouw regelgebaseerd (AI) systeem gecombineerd met een lerend (ML) systeem. Hierdoor is de sentimentanalyse volledig geautomatiseerd!\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <a href=\"https://www.aiopschool.be/chatbot/\"> \n",
    "        <img src=\"../_afbeeldingen/bannerugentdwengo.png\" alt=\"Dwengo\" style =\"display: block; margin-left: auto; margin-right: auto; margin-bottom: 30px; width:20%\"/>\n",
    "    </a>\n",
    "\n",
    "Deze Notebook is gebaseerd op: Notebook sentimentanalysemodel, zie <a href=\"http://www.aiopschool.be\">AI Op School</a>, van S. Pletinck , F. wyffels & N. Gesquière is in licentie gegeven volgens een <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Naamsvermelding-NietCommercieel-GelijkDelen 4.0 Internationaal-licentie</a>. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "b6c46c674276094c5b4947a85365e840b5f628ca41d1c53f265091d2c5ea301d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
